{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25841 images belonging to 7 classes.\n",
      "Found 2868 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636ms/step - accuracy: 0.1938 - loss: 2.4143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 672ms/step - accuracy: 0.1938 - loss: 2.4137 - val_accuracy: 0.1897 - val_loss: 3.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 398ms/step - accuracy: 0.2355 - loss: 2.0907 - val_accuracy: 0.2706 - val_loss: 2.1498 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 428ms/step - accuracy: 0.2485 - loss: 2.0498 - val_accuracy: 0.2894 - val_loss: 2.1964 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 439ms/step - accuracy: 0.2645 - loss: 2.0248 - val_accuracy: 0.2744 - val_loss: 2.1851 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 426ms/step - accuracy: 0.2643 - loss: 1.9913 - val_accuracy: 0.3020 - val_loss: 2.0281 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 444ms/step - accuracy: 0.2715 - loss: 1.9553 - val_accuracy: 0.3281 - val_loss: 1.8942 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 443ms/step - accuracy: 0.2936 - loss: 1.9126 - val_accuracy: 0.3330 - val_loss: 1.8663 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 442ms/step - accuracy: 0.2978 - loss: 1.8822 - val_accuracy: 0.3347 - val_loss: 1.8727 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 470ms/step - accuracy: 0.3146 - loss: 1.8371 - val_accuracy: 0.3459 - val_loss: 1.7533 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 450ms/step - accuracy: 0.3252 - loss: 1.7960 - val_accuracy: 0.3748 - val_loss: 1.7082 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 460ms/step - accuracy: 0.3378 - loss: 1.7577 - val_accuracy: 0.3825 - val_loss: 1.6574 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 431ms/step - accuracy: 0.3434 - loss: 1.7253 - val_accuracy: 0.3961 - val_loss: 1.6213 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 463ms/step - accuracy: 0.3554 - loss: 1.7019 - val_accuracy: 0.3971 - val_loss: 1.6122 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 478ms/step - accuracy: 0.3617 - loss: 1.6781 - val_accuracy: 0.3968 - val_loss: 1.5976 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 457ms/step - accuracy: 0.3699 - loss: 1.6459 - val_accuracy: 0.4142 - val_loss: 1.5481 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 432ms/step - accuracy: 0.3775 - loss: 1.6343 - val_accuracy: 0.4268 - val_loss: 1.5473 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 427ms/step - accuracy: 0.3858 - loss: 1.6123 - val_accuracy: 0.4121 - val_loss: 1.5314 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 439ms/step - accuracy: 0.3923 - loss: 1.6024 - val_accuracy: 0.4296 - val_loss: 1.4975 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 459ms/step - accuracy: 0.3885 - loss: 1.6007 - val_accuracy: 0.4439 - val_loss: 1.4869 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 431ms/step - accuracy: 0.4011 - loss: 1.5724 - val_accuracy: 0.4463 - val_loss: 1.4860 - learning_rate: 1.0000e-04\n",
      "Found 7178 images belonging to 7 classes.\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 284ms/step - accuracy: 0.4820 - loss: 1.4066\n",
      "Test Loss: 1.4183, Test Accuracy: 0.4728\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# 資料集路徑\n",
    "dataset_path = 'fer2013'\n",
    "train_dir = os.path.join(dataset_path, 'train')\n",
    "test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "# 資料預處理與數據增強\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.1  # 將訓練數據分為訓練集和驗證集\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # 修正為從訓練資料中劃分驗證集\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# 模型架構設計\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 訓練回調函數\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # 增大耐心值\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# 測試集驗證\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,  # 修正測試集路徑\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 評估模型\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集各類別樣本數量： {'angry': 3995, 'disgust': 436, 'fear': 4097, 'happy': 7215, 'neutral': 4965, 'sad': 4830, 'surprise': 3171}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# 訓練數據夾路徑\n",
    "train_classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "# 計算每個類別的樣本數量\n",
    "train_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in train_classes}\n",
    "\n",
    "print(\"訓練集各類別樣本數量：\", train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集各類別樣本數量： {'angry': 958, 'disgust': 111, 'fear': 1024, 'happy': 1774, 'neutral': 1233, 'sad': 1247, 'surprise': 831}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# 訓練數據夾路徑\n",
    "test_classes = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "\n",
    "# 計算每個類別的樣本數量\n",
    "test_counts = {cls: len(os.listdir(os.path.join(test_dir, cls))) for cls in test_classes}\n",
    "\n",
    "print(\"訓練集各類別樣本數量：\", test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 436 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# 數據增強設置\n",
    "augment_disgust = ImageDataGenerator(\n",
    "    rotation_range=40,         # 隨機旋轉\n",
    "    width_shift_range=0.2,     # 水平平移\n",
    "    height_shift_range=0.2,    # 垂直平移\n",
    "    zoom_range=0.2,            # 隨機縮放\n",
    "    horizontal_flip=True,      # 水平翻轉\n",
    "    fill_mode='nearest'        # 填充模式\n",
    ")\n",
    "\n",
    "# 增強數據保存到新路徑\n",
    "augmented_path = 'disgust_images/'  # 保存增強後的圖片\n",
    "disgust_generator = augment_disgust.flow_from_directory(\n",
    "    'fer2013/train',           # 指向父資料夾\n",
    "    classes=['disgust'],       # 指定 \"disgust\" 類別\n",
    "    target_size=(96, 96),      # 與模型輸入尺寸一致\n",
    "    batch_size=32,\n",
    "    save_to_dir=augmented_path,  # 保存增強後圖片\n",
    "    save_format='jpeg',\n",
    "    class_mode=None\n",
    ")\n",
    "\n",
    "# 控制增強次數（以批次計算）\n",
    "for i in range(125):  \n",
    "    next(disgust_generator)  # 使用內建 next() 方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集各類別樣本數量： {'angry': 3995, 'disgust': 4808, 'fear': 4097, 'happy': 7215, 'neutral': 4965, 'sad': 4830, 'surprise': 3171}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_path = 'fer2013/train'\n",
    "class_counts = {cls: len(os.listdir(os.path.join(train_path, cls))) for cls in os.listdir(train_path)}\n",
    "\n",
    "print(\"訓練集各類別樣本數量：\", class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 定義模型架構\n",
    "model = Sequential([\n",
    "    # 卷積層 1\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 卷積層 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 卷積層 3\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 全連接層\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # 輸出 7 個情緒分類\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29776 images belonging to 7 classes.\n",
      "Found 3305 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 訓練數據生成器（包括增強）\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,        # 像素值歸一化\n",
    "    rotation_range=30,          # 隨機旋轉\n",
    "    zoom_range=0.2,             # 隨機縮放\n",
    "    shear_range=0.2,            # 剪切變換\n",
    "    horizontal_flip=True,       # 水平翻轉\n",
    "    fill_mode='nearest',        # 填充模式\n",
    "    validation_split=0.1        # 劃分 10% 的數據作為驗證集\n",
    ")\n",
    "\n",
    "# 測試數據生成器（無增強）\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# 加載數據\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'fer2013/train',            # 訓練數據路徑\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',     # 灰階圖片\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training'           # 訓練集\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    'fer2013/train',\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'         # 驗證集\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'fer2013/test',             # 測試數據路徑\n",
    "    target_size=(96, 96),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 早停\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 學習率調整\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534ms/step - accuracy: 0.1967 - loss: 2.1592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 553ms/step - accuracy: 0.1968 - loss: 2.1588 - val_accuracy: 0.2148 - val_loss: 3.5631 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 479ms/step - accuracy: 0.2345 - loss: 1.8950 - val_accuracy: 0.2475 - val_loss: 2.2514 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 498ms/step - accuracy: 0.2819 - loss: 1.8136 - val_accuracy: 0.2405 - val_loss: 3.0524 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 441ms/step - accuracy: 0.3464 - loss: 1.6188 - val_accuracy: 0.2590 - val_loss: 2.4189 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 459ms/step - accuracy: 0.3667 - loss: 1.5273 - val_accuracy: 0.2923 - val_loss: 2.1565 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 441ms/step - accuracy: 0.3820 - loss: 1.4907 - val_accuracy: 0.2905 - val_loss: 2.6180 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 472ms/step - accuracy: 0.3901 - loss: 1.4658 - val_accuracy: 0.2762 - val_loss: 3.1609 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 514ms/step - accuracy: 0.4047 - loss: 1.4401 - val_accuracy: 0.3107 - val_loss: 2.5191 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 475ms/step - accuracy: 0.4173 - loss: 1.4097 - val_accuracy: 0.3041 - val_loss: 2.7614 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 484ms/step - accuracy: 0.4157 - loss: 1.4175 - val_accuracy: 0.3455 - val_loss: 2.2833 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 476ms/step - accuracy: 0.4303 - loss: 1.3791 - val_accuracy: 0.3359 - val_loss: 2.7969 - learning_rate: 5.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 511ms/step - accuracy: 0.4358 - loss: 1.3696 - val_accuracy: 0.3531 - val_loss: 2.7053 - learning_rate: 5.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 505ms/step - accuracy: 0.4496 - loss: 1.3586 - val_accuracy: 0.3631 - val_loss: 2.4043 - learning_rate: 5.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 477ms/step - accuracy: 0.4521 - loss: 1.3454 - val_accuracy: 0.3664 - val_loss: 2.6084 - learning_rate: 5.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m466/466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 481ms/step - accuracy: 0.4586 - loss: 1.3422 - val_accuracy: 0.3631 - val_loss: 2.1640 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,                        # 訓練世代數\n",
    "    validation_data=val_generator,    # 驗證數據\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試模型\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"測試集損失: {test_loss:.4f}\")\n",
    "print(f\"測試集準確率: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 可視化損失\n",
    "plt.plot(history.history['loss'], label='訓練損失')\n",
    "plt.plot(history.history['val_loss'], label='驗證損失')\n",
    "plt.legend()\n",
    "plt.title('訓練與驗證損失')\n",
    "plt.show()\n",
    "\n",
    "# 可視化準確率\n",
    "plt.plot(history.history['accuracy'], label='訓練準確率')\n",
    "plt.plot(history.history['val_accuracy'], label='驗證準確率')\n",
    "plt.legend()\n",
    "plt.title('訓練與驗證準確率')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1d2a9ecc905ef171fa753a7e952cc3d53b061ba87f364110eb501a5ad998ef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
