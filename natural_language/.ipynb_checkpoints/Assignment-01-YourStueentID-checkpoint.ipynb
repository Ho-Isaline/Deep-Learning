{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab534bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Before your go ----\n",
    "# 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "# 2. The deadline of Assignment-01 is 23:59pm, 03-23-2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24de0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this assignment, you will \n",
    "# 1) explore Wikipedia text data\n",
    "# 2) build language models\n",
    "# 3) build a Naive Bayes classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b666374b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (479847652.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [2], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    import enwiki-test.json\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Task0 Download the preprocessed data, enwiki-train.json \n",
    "# and enwiki-test from the Assignment-01 folder. In the data \n",
    "# file, each line contains a Wikipedia page with attributes, \n",
    "# title, label, and text. There are 1000 records in the train \n",
    "# file and 100 records in test file with ten categories.\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8383a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 1) Print out how many documents are in each class \n",
    "#    (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b63fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 2) Print out the average number of sentences in each class. \n",
    "# You may need to use sentence tokenization of NLTK.\n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4afd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 3) Print out the average number of tokens in each class \n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58546e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 4) For each sentence in the document, remove punctuations and \n",
    "# other special characters so that each sentence only contains \n",
    "# English words and numbers. To make your life easier, you can \n",
    "# make all words as lower cases. For each class, print out the \n",
    "# first article's name and the processed first 40 words.\n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7078af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 1) Based on the training dataset, build unigram, bigram, \n",
    "# and trigram language models using Add-one smoothing technique.\n",
    "# It is encouraged to implement models by yourself.\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 2) Report the perplexity of these 3 trained models on the \n",
    "# testing dataset and explain your findings. \n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 3) Use each built model to generate five sentences and \n",
    "# explain these generated patterns.\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a961249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 --- Build Naive Bayes classifiers --- \n",
    "# 1) Build Naive Bayes classifiers (with Laplace smoothing)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 --- Build Naive Bayes classifiers --- \n",
    "# 2) Report Micro-F1 score and Macro-F1 score for these \n",
    "# classifiers on testing dataset explain our results.\n",
    "\n",
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d2a9ecc905ef171fa753a7e952cc3d53b061ba87f364110eb501a5ad998ef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
