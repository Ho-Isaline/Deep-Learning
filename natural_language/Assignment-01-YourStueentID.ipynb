{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab534bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Before your go ----\n",
    "# 1. Rename Assignment-01-###.ipynb where ### is your student ID.\n",
    "# 2. The deadline of Assignment-01 is 23:59pm, 03-23-2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24de0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this assignment, you will \n",
    "# 1) explore Wikipedia text data\n",
    "# 2) build language models\n",
    "# 3) build a Naive Bayes classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b666374b",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 49395)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menwiki-test.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m test:\n\u001b[1;32m---> 12\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 49395)"
     ]
    }
   ],
   "source": [
    "# Task0 Download the preprocessed data, enwiki-train.json \n",
    "# and enwiki-test from the Assignment-01 folder. In the data \n",
    "# file, each line contains a Wikipedia page with attributes, \n",
    "# title, label, and text. There are 1000 records in the train \n",
    "# file and 100 records in test file with ten categories.\n",
    "\n",
    "# Your code\n",
    "\n",
    "import json\n",
    "\n",
    "with open('enwiki-test.json') as test:\n",
    "    test_data = json.load(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8383a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 1) Print out how many documents are in each class \n",
    "#    (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b63fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 2) Print out the average number of sentences in each class. \n",
    "# You may need to use sentence tokenization of NLTK.\n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4afd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 3) Print out the average number of tokens in each class \n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58546e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 --- Data exploring and preprocessing ---\n",
    "# 4) For each sentence in the document, remove punctuations and \n",
    "# other special characters so that each sentence only contains \n",
    "# English words and numbers. To make your life easier, you can \n",
    "# make all words as lower cases. For each class, print out the \n",
    "# first article's name and the processed first 40 words.\n",
    "# (for both train and test dataset)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7078af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 1) Based on the training dataset, build unigram, bigram, \n",
    "# and trigram language models using Add-one smoothing technique.\n",
    "# It is encouraged to implement models by yourself.\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 2) Report the perplexity of these 3 trained models on the \n",
    "# testing dataset and explain your findings. \n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 --- Build language models ---\n",
    "# 3) Use each built model to generate five sentences and \n",
    "# explain these generated patterns.\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a961249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 --- Build Naive Bayes classifiers --- \n",
    "# 1) Build Naive Bayes classifiers (with Laplace smoothing)\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 --- Build Naive Bayes classifiers --- \n",
    "# 2) Report Micro-F1 score and Macro-F1 score for these \n",
    "# classifiers on testing dataset explain our results.\n",
    "\n",
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d2a9ecc905ef171fa753a7e952cc3d53b061ba87f364110eb501a5ad998ef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
